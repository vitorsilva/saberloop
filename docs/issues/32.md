# Issue #32: Quiz prompt generates logically equivalent answer options

## Status: âœ… COMPLETE (Dec 26, 2025)

**PR**: #33 (merged)

## Problem Statement

Users report quiz questions where multiple answer options are scientifically correct because they describe the same fact from opposite perspectives (logical inverses).

**Example:**
- Option A: "Symmetric ions increase melting point"
- Option B: "Asymmetric ions decrease melting point" (marked correct)

Both are true - they're the same relationship stated differently.

## Root Cause Analysis

The question generation prompt in `src/api/api.real.js:21-61` lacks explicit guidance to:
1. Ensure distractors are genuinely incorrect (not just alternative phrasings)
2. Avoid logical inverse pairs that are both technically correct

## Solution Design

Add new requirements to the prompt instructing the LLM to:
- Make each option represent a distinct claim
- Ensure wrong answers are factually incorrect
- Avoid logical inverse pairs
- Have each distractor test a different misconception

## Files to Change

| File | Change |
|------|--------|
| `src/api/api.real.js` | Add distractor quality guidance to prompt (lines 34-41) |

## Testing Plan

- [x] Unit test: Verify prompt contains new guidance text
- [x] Manual test: Generate quizzes on 5+ topics, verify options are distinct
- [x] Regression: Existing E2E tests pass
- [x] Regression: Build succeeds

## Implementation Steps

- [x] 1. Add distractor quality requirements to prompt
- [x] 2. Write unit test for prompt content
- [x] 3. Run tests and verify (127 unit tests pass, build succeeds)
- [x] 4. Manual testing with various topics
- [x] 5. Create PR (#33, merged)

## Notes

This is a prompt engineering fix. Since LLM output is non-deterministic, we cannot write a test that guarantees the issue won't occur. The test will verify the prompt contains the guidance; actual quality improvement relies on the LLM following instructions.
