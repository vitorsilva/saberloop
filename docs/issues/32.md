# Issue #32: Quiz prompt generates logically equivalent answer options

## Problem Statement

Users report quiz questions where multiple answer options are scientifically correct because they describe the same fact from opposite perspectives (logical inverses).

**Example:**
- Option A: "Symmetric ions increase melting point"
- Option B: "Asymmetric ions decrease melting point" (marked correct)

Both are true - they're the same relationship stated differently.

## Root Cause Analysis

The question generation prompt in `src/api/api.real.js:21-61` lacks explicit guidance to:
1. Ensure distractors are genuinely incorrect (not just alternative phrasings)
2. Avoid logical inverse pairs that are both technically correct

## Solution Design

Add new requirements to the prompt instructing the LLM to:
- Make each option represent a distinct claim
- Ensure wrong answers are factually incorrect
- Avoid logical inverse pairs
- Have each distractor test a different misconception

## Files to Change

| File | Change |
|------|--------|
| `src/api/api.real.js` | Add distractor quality guidance to prompt (lines 34-41) |

## Testing Plan

- [ ] Unit test: Verify prompt contains new guidance text
- [ ] Manual test: Generate quizzes on 5+ topics, verify options are distinct
- [ ] Regression: Existing E2E tests pass
- [ ] Regression: Build succeeds

## Implementation Steps

- [ ] 1. Add distractor quality requirements to prompt
- [ ] 2. Write unit test for prompt content
- [ ] 3. Run tests and verify
- [ ] 4. Manual testing with various topics
- [ ] 5. Create PR

## Notes

This is a prompt engineering fix. Since LLM output is non-deterministic, we cannot write a test that guarantees the issue won't occur. The test will verify the prompt contains the guidance; actual quality improvement relies on the LLM following instructions.
